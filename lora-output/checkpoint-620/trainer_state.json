{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 620,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008064516129032258,
      "grad_norm": 8.400646209716797,
      "learning_rate": 4.967741935483871e-05,
      "loss": 14.0442,
      "step": 5
    },
    {
      "epoch": 0.016129032258064516,
      "grad_norm": 11.4507417678833,
      "learning_rate": 4.92741935483871e-05,
      "loss": 13.9941,
      "step": 10
    },
    {
      "epoch": 0.024193548387096774,
      "grad_norm": 12.901204109191895,
      "learning_rate": 4.887096774193549e-05,
      "loss": 12.9405,
      "step": 15
    },
    {
      "epoch": 0.03225806451612903,
      "grad_norm": 15.687910079956055,
      "learning_rate": 4.846774193548387e-05,
      "loss": 12.3752,
      "step": 20
    },
    {
      "epoch": 0.04032258064516129,
      "grad_norm": 20.3787899017334,
      "learning_rate": 4.806451612903226e-05,
      "loss": 11.6934,
      "step": 25
    },
    {
      "epoch": 0.04838709677419355,
      "grad_norm": 111.58394622802734,
      "learning_rate": 4.766129032258065e-05,
      "loss": 9.861,
      "step": 30
    },
    {
      "epoch": 0.056451612903225805,
      "grad_norm": 131.82891845703125,
      "learning_rate": 4.725806451612904e-05,
      "loss": 8.4426,
      "step": 35
    },
    {
      "epoch": 0.06451612903225806,
      "grad_norm": 145.7461700439453,
      "learning_rate": 4.685483870967742e-05,
      "loss": 5.72,
      "step": 40
    },
    {
      "epoch": 0.07258064516129033,
      "grad_norm": 203.3072967529297,
      "learning_rate": 4.645161290322581e-05,
      "loss": 4.0183,
      "step": 45
    },
    {
      "epoch": 0.08064516129032258,
      "grad_norm": 178.8570098876953,
      "learning_rate": 4.60483870967742e-05,
      "loss": 5.5129,
      "step": 50
    },
    {
      "epoch": 0.08870967741935484,
      "grad_norm": 78.4822006225586,
      "learning_rate": 4.5645161290322584e-05,
      "loss": 4.7735,
      "step": 55
    },
    {
      "epoch": 0.0967741935483871,
      "grad_norm": 75.05844116210938,
      "learning_rate": 4.5241935483870966e-05,
      "loss": 5.4588,
      "step": 60
    },
    {
      "epoch": 0.10483870967741936,
      "grad_norm": 747.1768798828125,
      "learning_rate": 4.4838709677419356e-05,
      "loss": 5.3476,
      "step": 65
    },
    {
      "epoch": 0.11290322580645161,
      "grad_norm": 235.87991333007812,
      "learning_rate": 4.4435483870967745e-05,
      "loss": 4.9714,
      "step": 70
    },
    {
      "epoch": 0.12096774193548387,
      "grad_norm": 188.42135620117188,
      "learning_rate": 4.403225806451613e-05,
      "loss": 4.6214,
      "step": 75
    },
    {
      "epoch": 0.12903225806451613,
      "grad_norm": 118.44676971435547,
      "learning_rate": 4.362903225806452e-05,
      "loss": 4.0993,
      "step": 80
    },
    {
      "epoch": 0.13709677419354838,
      "grad_norm": 60.09230422973633,
      "learning_rate": 4.322580645161291e-05,
      "loss": 3.6949,
      "step": 85
    },
    {
      "epoch": 0.14516129032258066,
      "grad_norm": 56.60321044921875,
      "learning_rate": 4.282258064516129e-05,
      "loss": 2.4972,
      "step": 90
    },
    {
      "epoch": 0.1532258064516129,
      "grad_norm": 69.08489990234375,
      "learning_rate": 4.241935483870968e-05,
      "loss": 3.0955,
      "step": 95
    },
    {
      "epoch": 0.16129032258064516,
      "grad_norm": 122.52698516845703,
      "learning_rate": 4.201612903225807e-05,
      "loss": 2.5812,
      "step": 100
    },
    {
      "epoch": 0.1693548387096774,
      "grad_norm": 47.74557113647461,
      "learning_rate": 4.161290322580645e-05,
      "loss": 1.2276,
      "step": 105
    },
    {
      "epoch": 0.1774193548387097,
      "grad_norm": 50.105995178222656,
      "learning_rate": 4.120967741935484e-05,
      "loss": 1.5186,
      "step": 110
    },
    {
      "epoch": 0.18548387096774194,
      "grad_norm": 15.159125328063965,
      "learning_rate": 4.080645161290323e-05,
      "loss": 1.3759,
      "step": 115
    },
    {
      "epoch": 0.1935483870967742,
      "grad_norm": 31.934898376464844,
      "learning_rate": 4.0403225806451614e-05,
      "loss": 1.4757,
      "step": 120
    },
    {
      "epoch": 0.20161290322580644,
      "grad_norm": 19.668270111083984,
      "learning_rate": 4e-05,
      "loss": 0.9168,
      "step": 125
    },
    {
      "epoch": 0.20967741935483872,
      "grad_norm": 13.759902000427246,
      "learning_rate": 3.959677419354839e-05,
      "loss": 1.2133,
      "step": 130
    },
    {
      "epoch": 0.21774193548387097,
      "grad_norm": 57.46158981323242,
      "learning_rate": 3.9193548387096776e-05,
      "loss": 1.1516,
      "step": 135
    },
    {
      "epoch": 0.22580645161290322,
      "grad_norm": 148.53749084472656,
      "learning_rate": 3.879032258064516e-05,
      "loss": 0.9754,
      "step": 140
    },
    {
      "epoch": 0.23387096774193547,
      "grad_norm": 2.479799509048462,
      "learning_rate": 3.838709677419355e-05,
      "loss": 0.819,
      "step": 145
    },
    {
      "epoch": 0.24193548387096775,
      "grad_norm": 2.2827024459838867,
      "learning_rate": 3.798387096774194e-05,
      "loss": 0.6863,
      "step": 150
    },
    {
      "epoch": 0.25,
      "grad_norm": 2.101516008377075,
      "learning_rate": 3.758064516129032e-05,
      "loss": 1.0499,
      "step": 155
    },
    {
      "epoch": 0.25806451612903225,
      "grad_norm": 1.8324828147888184,
      "learning_rate": 3.717741935483871e-05,
      "loss": 0.9804,
      "step": 160
    },
    {
      "epoch": 0.2661290322580645,
      "grad_norm": 215.76646423339844,
      "learning_rate": 3.67741935483871e-05,
      "loss": 1.0073,
      "step": 165
    },
    {
      "epoch": 0.27419354838709675,
      "grad_norm": 1.201357364654541,
      "learning_rate": 3.637096774193549e-05,
      "loss": 1.0619,
      "step": 170
    },
    {
      "epoch": 0.28225806451612906,
      "grad_norm": 1.4086867570877075,
      "learning_rate": 3.596774193548387e-05,
      "loss": 0.8873,
      "step": 175
    },
    {
      "epoch": 0.2903225806451613,
      "grad_norm": 0.8465936779975891,
      "learning_rate": 3.556451612903226e-05,
      "loss": 0.6479,
      "step": 180
    },
    {
      "epoch": 0.29838709677419356,
      "grad_norm": 116.37939453125,
      "learning_rate": 3.516129032258065e-05,
      "loss": 0.6639,
      "step": 185
    },
    {
      "epoch": 0.3064516129032258,
      "grad_norm": 188.4681396484375,
      "learning_rate": 3.475806451612903e-05,
      "loss": 1.2429,
      "step": 190
    },
    {
      "epoch": 0.31451612903225806,
      "grad_norm": 110.78157043457031,
      "learning_rate": 3.435483870967742e-05,
      "loss": 1.2597,
      "step": 195
    },
    {
      "epoch": 0.3225806451612903,
      "grad_norm": 10.005223274230957,
      "learning_rate": 3.395161290322581e-05,
      "loss": 0.8155,
      "step": 200
    },
    {
      "epoch": 0.33064516129032256,
      "grad_norm": 214.09829711914062,
      "learning_rate": 3.3548387096774195e-05,
      "loss": 1.209,
      "step": 205
    },
    {
      "epoch": 0.3387096774193548,
      "grad_norm": 9.066059112548828,
      "learning_rate": 3.3145161290322585e-05,
      "loss": 0.7627,
      "step": 210
    },
    {
      "epoch": 0.3467741935483871,
      "grad_norm": 1.6264944076538086,
      "learning_rate": 3.274193548387097e-05,
      "loss": 0.5131,
      "step": 215
    },
    {
      "epoch": 0.3548387096774194,
      "grad_norm": 1.072716474533081,
      "learning_rate": 3.233870967741936e-05,
      "loss": 0.6512,
      "step": 220
    },
    {
      "epoch": 0.3629032258064516,
      "grad_norm": 0.8551445007324219,
      "learning_rate": 3.193548387096774e-05,
      "loss": 0.68,
      "step": 225
    },
    {
      "epoch": 0.3709677419354839,
      "grad_norm": 32.6523551940918,
      "learning_rate": 3.153225806451613e-05,
      "loss": 0.9083,
      "step": 230
    },
    {
      "epoch": 0.3790322580645161,
      "grad_norm": 2.0376760959625244,
      "learning_rate": 3.112903225806452e-05,
      "loss": 1.1865,
      "step": 235
    },
    {
      "epoch": 0.3870967741935484,
      "grad_norm": 1.7388678789138794,
      "learning_rate": 3.07258064516129e-05,
      "loss": 1.1418,
      "step": 240
    },
    {
      "epoch": 0.3951612903225806,
      "grad_norm": 2.3494033813476562,
      "learning_rate": 3.032258064516129e-05,
      "loss": 0.5454,
      "step": 245
    },
    {
      "epoch": 0.4032258064516129,
      "grad_norm": 207.7099609375,
      "learning_rate": 2.9919354838709677e-05,
      "loss": 0.8203,
      "step": 250
    },
    {
      "epoch": 0.4112903225806452,
      "grad_norm": 6.148160934448242,
      "learning_rate": 2.9516129032258067e-05,
      "loss": 0.8452,
      "step": 255
    },
    {
      "epoch": 0.41935483870967744,
      "grad_norm": 1.4644726514816284,
      "learning_rate": 2.9112903225806453e-05,
      "loss": 0.5969,
      "step": 260
    },
    {
      "epoch": 0.4274193548387097,
      "grad_norm": 5.6146063804626465,
      "learning_rate": 2.8709677419354843e-05,
      "loss": 0.6021,
      "step": 265
    },
    {
      "epoch": 0.43548387096774194,
      "grad_norm": 0.6113423109054565,
      "learning_rate": 2.830645161290323e-05,
      "loss": 0.5257,
      "step": 270
    },
    {
      "epoch": 0.4435483870967742,
      "grad_norm": 2.644137144088745,
      "learning_rate": 2.7903225806451615e-05,
      "loss": 0.7515,
      "step": 275
    },
    {
      "epoch": 0.45161290322580644,
      "grad_norm": 1.36875581741333,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.6768,
      "step": 280
    },
    {
      "epoch": 0.4596774193548387,
      "grad_norm": 118.26439666748047,
      "learning_rate": 2.709677419354839e-05,
      "loss": 0.6162,
      "step": 285
    },
    {
      "epoch": 0.46774193548387094,
      "grad_norm": 1.5953752994537354,
      "learning_rate": 2.6693548387096773e-05,
      "loss": 0.4936,
      "step": 290
    },
    {
      "epoch": 0.47580645161290325,
      "grad_norm": 130.2874755859375,
      "learning_rate": 2.629032258064516e-05,
      "loss": 0.6855,
      "step": 295
    },
    {
      "epoch": 0.4838709677419355,
      "grad_norm": 8.979634284973145,
      "learning_rate": 2.588709677419355e-05,
      "loss": 0.6956,
      "step": 300
    },
    {
      "epoch": 0.49193548387096775,
      "grad_norm": 0.6035264134407043,
      "learning_rate": 2.5483870967741935e-05,
      "loss": 0.8033,
      "step": 305
    },
    {
      "epoch": 0.5,
      "grad_norm": 5.375003337860107,
      "learning_rate": 2.508064516129032e-05,
      "loss": 0.6081,
      "step": 310
    },
    {
      "epoch": 0.5080645161290323,
      "grad_norm": 0.8358314037322998,
      "learning_rate": 2.467741935483871e-05,
      "loss": 0.7453,
      "step": 315
    },
    {
      "epoch": 0.5161290322580645,
      "grad_norm": 0.6679100394248962,
      "learning_rate": 2.4274193548387097e-05,
      "loss": 0.6448,
      "step": 320
    },
    {
      "epoch": 0.5241935483870968,
      "grad_norm": 1.7889320850372314,
      "learning_rate": 2.3870967741935486e-05,
      "loss": 0.658,
      "step": 325
    },
    {
      "epoch": 0.532258064516129,
      "grad_norm": 0.9867084622383118,
      "learning_rate": 2.3467741935483873e-05,
      "loss": 0.5433,
      "step": 330
    },
    {
      "epoch": 0.5403225806451613,
      "grad_norm": 0.5373169183731079,
      "learning_rate": 2.306451612903226e-05,
      "loss": 0.6149,
      "step": 335
    },
    {
      "epoch": 0.5483870967741935,
      "grad_norm": 8.043347358703613,
      "learning_rate": 2.266129032258065e-05,
      "loss": 0.7188,
      "step": 340
    },
    {
      "epoch": 0.5564516129032258,
      "grad_norm": 0.5822139978408813,
      "learning_rate": 2.2258064516129034e-05,
      "loss": 0.4729,
      "step": 345
    },
    {
      "epoch": 0.5645161290322581,
      "grad_norm": 8.846065521240234,
      "learning_rate": 2.185483870967742e-05,
      "loss": 0.3418,
      "step": 350
    },
    {
      "epoch": 0.5725806451612904,
      "grad_norm": 0.9357274174690247,
      "learning_rate": 2.1451612903225807e-05,
      "loss": 0.8144,
      "step": 355
    },
    {
      "epoch": 0.5806451612903226,
      "grad_norm": 1.0655736923217773,
      "learning_rate": 2.1048387096774193e-05,
      "loss": 0.5602,
      "step": 360
    },
    {
      "epoch": 0.5887096774193549,
      "grad_norm": 5.766241550445557,
      "learning_rate": 2.0645161290322582e-05,
      "loss": 0.6297,
      "step": 365
    },
    {
      "epoch": 0.5967741935483871,
      "grad_norm": 0.5789722800254822,
      "learning_rate": 2.024193548387097e-05,
      "loss": 0.3476,
      "step": 370
    },
    {
      "epoch": 0.6048387096774194,
      "grad_norm": 4.284086227416992,
      "learning_rate": 1.9838709677419358e-05,
      "loss": 0.7422,
      "step": 375
    },
    {
      "epoch": 0.6129032258064516,
      "grad_norm": 3.233978748321533,
      "learning_rate": 1.9435483870967744e-05,
      "loss": 0.6085,
      "step": 380
    },
    {
      "epoch": 0.6209677419354839,
      "grad_norm": 1.5087413787841797,
      "learning_rate": 1.9032258064516127e-05,
      "loss": 0.4226,
      "step": 385
    },
    {
      "epoch": 0.6290322580645161,
      "grad_norm": 0.9210741519927979,
      "learning_rate": 1.8629032258064517e-05,
      "loss": 0.3572,
      "step": 390
    },
    {
      "epoch": 0.6370967741935484,
      "grad_norm": 0.4431213438510895,
      "learning_rate": 1.8225806451612903e-05,
      "loss": 0.6481,
      "step": 395
    },
    {
      "epoch": 0.6451612903225806,
      "grad_norm": 2.439175844192505,
      "learning_rate": 1.7822580645161292e-05,
      "loss": 0.7217,
      "step": 400
    },
    {
      "epoch": 0.6532258064516129,
      "grad_norm": 0.645913302898407,
      "learning_rate": 1.741935483870968e-05,
      "loss": 0.4205,
      "step": 405
    },
    {
      "epoch": 0.6612903225806451,
      "grad_norm": 1.540509819984436,
      "learning_rate": 1.7016129032258065e-05,
      "loss": 0.547,
      "step": 410
    },
    {
      "epoch": 0.6693548387096774,
      "grad_norm": 160.78451538085938,
      "learning_rate": 1.6612903225806454e-05,
      "loss": 0.6555,
      "step": 415
    },
    {
      "epoch": 0.6774193548387096,
      "grad_norm": 1.4025816917419434,
      "learning_rate": 1.620967741935484e-05,
      "loss": 0.4528,
      "step": 420
    },
    {
      "epoch": 0.6854838709677419,
      "grad_norm": 2.1406519412994385,
      "learning_rate": 1.5806451612903226e-05,
      "loss": 0.5151,
      "step": 425
    },
    {
      "epoch": 0.6935483870967742,
      "grad_norm": 0.6934581398963928,
      "learning_rate": 1.5403225806451613e-05,
      "loss": 0.5037,
      "step": 430
    },
    {
      "epoch": 0.7016129032258065,
      "grad_norm": 0.646665632724762,
      "learning_rate": 1.5e-05,
      "loss": 0.4528,
      "step": 435
    },
    {
      "epoch": 0.7096774193548387,
      "grad_norm": 0.7961811423301697,
      "learning_rate": 1.4596774193548388e-05,
      "loss": 0.6243,
      "step": 440
    },
    {
      "epoch": 0.717741935483871,
      "grad_norm": 0.850111722946167,
      "learning_rate": 1.4193548387096774e-05,
      "loss": 0.3603,
      "step": 445
    },
    {
      "epoch": 0.7258064516129032,
      "grad_norm": 149.3466033935547,
      "learning_rate": 1.3790322580645162e-05,
      "loss": 0.601,
      "step": 450
    },
    {
      "epoch": 0.7338709677419355,
      "grad_norm": 0.569508969783783,
      "learning_rate": 1.338709677419355e-05,
      "loss": 0.3817,
      "step": 455
    },
    {
      "epoch": 0.7419354838709677,
      "grad_norm": 0.9619576334953308,
      "learning_rate": 1.2983870967741938e-05,
      "loss": 0.6261,
      "step": 460
    },
    {
      "epoch": 0.75,
      "grad_norm": 1.6844215393066406,
      "learning_rate": 1.2580645161290322e-05,
      "loss": 0.6121,
      "step": 465
    },
    {
      "epoch": 0.7580645161290323,
      "grad_norm": 0.8616889119148254,
      "learning_rate": 1.217741935483871e-05,
      "loss": 0.6425,
      "step": 470
    },
    {
      "epoch": 0.7661290322580645,
      "grad_norm": 1.6736514568328857,
      "learning_rate": 1.1774193548387096e-05,
      "loss": 0.8863,
      "step": 475
    },
    {
      "epoch": 0.7741935483870968,
      "grad_norm": 1.2081632614135742,
      "learning_rate": 1.1370967741935484e-05,
      "loss": 0.577,
      "step": 480
    },
    {
      "epoch": 0.782258064516129,
      "grad_norm": 0.8267799019813538,
      "learning_rate": 1.0967741935483872e-05,
      "loss": 0.5532,
      "step": 485
    },
    {
      "epoch": 0.7903225806451613,
      "grad_norm": 1.1016976833343506,
      "learning_rate": 1.056451612903226e-05,
      "loss": 0.6059,
      "step": 490
    },
    {
      "epoch": 0.7983870967741935,
      "grad_norm": 0.5997068881988525,
      "learning_rate": 1.0161290322580646e-05,
      "loss": 0.3259,
      "step": 495
    },
    {
      "epoch": 0.8064516129032258,
      "grad_norm": 1.6348137855529785,
      "learning_rate": 9.758064516129032e-06,
      "loss": 0.5383,
      "step": 500
    },
    {
      "epoch": 0.8145161290322581,
      "grad_norm": 0.8761245012283325,
      "learning_rate": 9.35483870967742e-06,
      "loss": 0.5738,
      "step": 505
    },
    {
      "epoch": 0.8225806451612904,
      "grad_norm": 0.8187909722328186,
      "learning_rate": 8.951612903225808e-06,
      "loss": 0.699,
      "step": 510
    },
    {
      "epoch": 0.8306451612903226,
      "grad_norm": 2.3003499507904053,
      "learning_rate": 8.548387096774194e-06,
      "loss": 0.6056,
      "step": 515
    },
    {
      "epoch": 0.8387096774193549,
      "grad_norm": 0.8569314479827881,
      "learning_rate": 8.145161290322582e-06,
      "loss": 0.6124,
      "step": 520
    },
    {
      "epoch": 0.8467741935483871,
      "grad_norm": 1.0093799829483032,
      "learning_rate": 7.741935483870968e-06,
      "loss": 0.522,
      "step": 525
    },
    {
      "epoch": 0.8548387096774194,
      "grad_norm": 1.0026285648345947,
      "learning_rate": 7.338709677419354e-06,
      "loss": 0.5562,
      "step": 530
    },
    {
      "epoch": 0.8629032258064516,
      "grad_norm": 1.379638433456421,
      "learning_rate": 6.935483870967742e-06,
      "loss": 0.4698,
      "step": 535
    },
    {
      "epoch": 0.8709677419354839,
      "grad_norm": 1.101915717124939,
      "learning_rate": 6.532258064516129e-06,
      "loss": 0.2157,
      "step": 540
    },
    {
      "epoch": 0.8790322580645161,
      "grad_norm": 0.9966777563095093,
      "learning_rate": 6.129032258064516e-06,
      "loss": 0.4899,
      "step": 545
    },
    {
      "epoch": 0.8870967741935484,
      "grad_norm": 1.1131335496902466,
      "learning_rate": 5.725806451612904e-06,
      "loss": 0.3487,
      "step": 550
    },
    {
      "epoch": 0.8951612903225806,
      "grad_norm": 2.674001693725586,
      "learning_rate": 5.32258064516129e-06,
      "loss": 0.5395,
      "step": 555
    },
    {
      "epoch": 0.9032258064516129,
      "grad_norm": 0.8126382231712341,
      "learning_rate": 4.919354838709678e-06,
      "loss": 0.4999,
      "step": 560
    },
    {
      "epoch": 0.9112903225806451,
      "grad_norm": 0.849933385848999,
      "learning_rate": 4.516129032258065e-06,
      "loss": 0.4817,
      "step": 565
    },
    {
      "epoch": 0.9193548387096774,
      "grad_norm": 0.9268113970756531,
      "learning_rate": 4.112903225806452e-06,
      "loss": 0.3987,
      "step": 570
    },
    {
      "epoch": 0.9274193548387096,
      "grad_norm": 0.9885553121566772,
      "learning_rate": 3.709677419354839e-06,
      "loss": 0.5551,
      "step": 575
    },
    {
      "epoch": 0.9354838709677419,
      "grad_norm": 1.0021530389785767,
      "learning_rate": 3.3064516129032262e-06,
      "loss": 0.2795,
      "step": 580
    },
    {
      "epoch": 0.9435483870967742,
      "grad_norm": 0.7412712574005127,
      "learning_rate": 2.903225806451613e-06,
      "loss": 0.5192,
      "step": 585
    },
    {
      "epoch": 0.9516129032258065,
      "grad_norm": 2.5128848552703857,
      "learning_rate": 2.5e-06,
      "loss": 0.588,
      "step": 590
    },
    {
      "epoch": 0.9596774193548387,
      "grad_norm": 0.8604357242584229,
      "learning_rate": 2.096774193548387e-06,
      "loss": 0.419,
      "step": 595
    },
    {
      "epoch": 0.967741935483871,
      "grad_norm": 0.7625998258590698,
      "learning_rate": 1.6935483870967744e-06,
      "loss": 0.3262,
      "step": 600
    },
    {
      "epoch": 0.9758064516129032,
      "grad_norm": 1.1493712663650513,
      "learning_rate": 1.2903225806451614e-06,
      "loss": 0.4809,
      "step": 605
    },
    {
      "epoch": 0.9838709677419355,
      "grad_norm": 0.9665917754173279,
      "learning_rate": 8.870967741935484e-07,
      "loss": 0.5524,
      "step": 610
    },
    {
      "epoch": 0.9919354838709677,
      "grad_norm": 0.9802448749542236,
      "learning_rate": 4.838709677419355e-07,
      "loss": 0.6102,
      "step": 615
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.9114086627960205,
      "learning_rate": 8.064516129032259e-08,
      "loss": 0.9034,
      "step": 620
    }
  ],
  "logging_steps": 5,
  "max_steps": 620,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1471147592908800.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
